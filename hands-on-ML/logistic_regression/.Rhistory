dplyr::sample_n(post,10)
results <- dplyr::select(post, -obs_index, -chain_index)
colMeans(results)
help(dbinom)
dbinom(1,1,prob=1/13)
1/13
dbinom(2,4,prob=1/13)
rm(list=ls())
install.packages("sqldf")
library(sqldf)
install.packages("C:/Users/hongjie/Desktop/sqldf_0.4-11.zip", repos = NULL, type = "win.binary")
ames <- AmesHousing::make_ames()
install.packages("AmesHousing")
ames <- AmesHousing::make_ames()
str(ames)
x=seq(1,10)
rsampling::bootstraps(x,times=2)
rsample::bootstraps(x,times=2)
install.packages("rsample")
rsample::bootstraps(x,times=2)
library(dplyr)
y<-rsample::bootstraps(x,times=2)
y
print(y)
x=seq(1,10)
mydata=data.frame(x)
mydata
y<-rsample::bootstraps(mydata,times=2)
y
y[1,1]
y$id=1
y$id=1
y
library(dplyr)
library(rsample)
rm(list=ls())
library(dplyr)
library(rsample)
ames <- AmesHousing::make_ames()
rm(list=ls())
library(dplyr)
library(rsample)
ames <- AmesHousing::make_ames()
set.seed(123)
split<-initial_split(ams,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price)
rm(list=ls())
library(dplyr)
library(rsample)
ames <- AmesHousing::make_ames()
set.seed(123)
split<-initial_split(ams,prop=0.7,strata="Sale_Price")
rm(list=ls())
library(dplyr)
library(rsample)
ames <- AmesHousing::make_ames()
set.seed(123)
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price)
density(ame_train$Sale_Price) %>%plot
lines(density(ame_test$Sale_Price))
help("expand.grid")
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
hyper_grid
summary(hyper_grid)
dim(hyper_grid)
hyper_grid(1,1)
hyper_grid[1,1]
hyper_grid[1,]
hyper_grid[2,]
hyper_grid[1:2,]
hyper_grid[1:20,]
hyper_grid
install.packages("ggplot2")
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
ames <- AmesHousing::make_ames()
set.seed(123)
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price) %>%plot
lines(density(ame_test$Sale_Price))
cv<-trainControl(
method="repeatedcv",
number=10,
repeats=5
)
# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
knn_fit<-train(
Sale_Price=.,
data=ame_train,
method="knn",
trControl=cv,
tuneGrid=hype_grid,
metric="RMSE"
)
knn_fit
ggplot(knn_fit)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
ames <- AmesHousing::make_ames()
set.seed(123)
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price) %>%plot
lines(density(ame_test$Sale_Price))
cv<-trainControl(
method="repeatedcv",
number=10,
repeats=5
)
# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
knn_fit<-train(
Sale_Price~.,
data=ame_train,
method="knn",
trControl=cv,
tuneGrid=hype_grid,
metric="RMSE"
)
knn_fit
ggplot(knn_fit)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
ames <- AmesHousing::make_ames()
set.seed(123)
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price) %>%plot
lines(density(ame_test$Sale_Price))
cv<-trainControl(
method="repeatedcv",
number=10,
repeats=5
)
# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
knn_fit<-train(
Sale_Price~.,
data=ame_train,
method="knn",
trControl=cv,
tuneGrid=hyper_grid,
metric="RMSE"
)
knn_fit
ggplot(knn_fit)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
set.seed(123)
ames <- AmesHousing::make_ames()
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price) %>%plot
lines(density(ame_test$Sale_Price))
help(count)
count(ames_train,Neighborhood)
count(ame_train,Neighborhood)
help("prop.table")
prop.table(count(ame_train,Neighborhood))
count(ame_train,Neighborhood)
test<-count(ame_train,Neighborhood)
str(test)
test[1,1]
head(test)
tabulate(ame_train$Neighborhood)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
library(recipes)
set.seed(123)
ames <- AmesHousing::make_ames()
# use rsample pacakge to stratify
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price) %>%plot
lines(density(ame_test$Sale_Price))
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
library(recipes)
set.seed(123)
ames <- AmesHousing::make_ames()
# use rsample pacakge to stratify
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ame_train<-training(split)
ame_test<-testing(split)
density(ame_train$Sale_Price) %>%plot
lines(density(ame_test$Sale_Price))
blueprint<-recipe(Sale_Price~.,data=ames_train) %>%
step_nzv(all_normal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(),-all_outcomes())%>%
step_scale(all_numeric(),-all_outcomes()) %>%
step_dummy(all_nominal(),-all_outcomes(),one_hot=TRUE)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
library(recipes)
set.seed(123)
ames <- AmesHousing::make_ames()
# use rsample pacakge to stratify
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ames_train<-training(split)
ames_test<-testing(split)
density(ames_train$Sale_Price) %>%plot
lines(density(ames_test$Sale_Price))
blueprint<-recipe(Sale_Price~.,data=ames_train) %>%
step_nzv(all_normal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(),-all_outcomes())%>%
step_scale(all_numeric(),-all_outcomes()) %>%
step_dummy(all_nominal(),-all_outcomes(),one_hot=TRUE)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
library(recipes)
set.seed(123)
ames <- AmesHousing::make_ames()
# use rsample pacakge to stratify
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ames_train<-training(split)
ames_test<-testing(split)
density(ames_train$Sale_Price) %>%plot
lines(density(ames_test$Sale_Price))
# use reciple pacakge to set up pre-processing blueprint
# specifiy the outcome var, nzv remove cat var with high concentration
blueprint<-recipe(Sale_Price~.,data=ames_train) %>%
step_nzv(all_normal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(),-all_outcomes())%>%
step_scale(all_numeric(),-all_outcomes()) %>%
step_dummy(all_nominal(),-all_outcomes(),one_hot=TRUE)
cv<-trainControl(
method="repeatedcv"
number=10,
repeats=5
)
hyper_grid<-expand.grid(k=seq(2,25,by=1))
knn_fit<-train(
blueprint,
data=ames_train,
method="knn",
trControl = cv,
tuneGrid = hyper_grid,
metric="RMSE"
)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
library(recipes)
set.seed(123)
ames <- AmesHousing::make_ames()
# use rsample pacakge to stratify
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ames_train<-training(split)
ames_test<-testing(split)
density(ames_train$Sale_Price) %>%plot
lines(density(ames_test$Sale_Price))
# use reciple pacakge to set up pre-processing blueprint
# specifiy the outcome var, nzv remove cat var with high concentration
blueprint<-recipe(Sale_Price~.,data=ames_train) %>%
step_nzv(all_normal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(),-all_outcomes())%>%
step_scale(all_numeric(),-all_outcomes()) %>%
step_dummy(all_nominal(),-all_outcomes(),one_hot=TRUE)
cv<-trainControl(
method="repeatedcv",
number=10,
repeats=5
)
hyper_grid<-expand.grid(k=seq(2,25,by=1))
knn_fit<-train(
blueprint,
data=ames_train,
method="knn",
trControl = cv,
tuneGrid = hyper_grid,
metric="RMSE"
)
rm(list=ls())
library(dplyr)
library(rsample)
library(caret)
library(ggplot2)
library(recipes)
set.seed(123)
ames <- AmesHousing::make_ames()
# use rsample pacakge to stratify
split<-initial_split(ames,prop=0.7,strata="Sale_Price")
ames_train<-training(split)
ames_test<-testing(split)
density(ames_train$Sale_Price) %>%plot
lines(density(ames_test$Sale_Price))
# use reciple pacakge to set up pre-processing blueprint
# specifiy the outcome var, nzv remove cat var with high concentration
blueprint<-recipe(Sale_Price~.,data=ames_train) %>%
step_nzv(all_nominal()) %>%
step_integer(matches("Qual|Cond|QC|Qu")) %>%
step_center(all_numeric(),-all_outcomes())%>%
step_scale(all_numeric(),-all_outcomes()) %>%
step_dummy(all_nominal(),-all_outcomes(),one_hot=TRUE)
cv<-trainControl(
method="repeatedcv",
number=10,
repeats=5
)
hyper_grid<-expand.grid(k=seq(2,25,by=1))
knn_fit<-train(
blueprint,
data=ames_train,
method="knn",
trControl = cv,
tuneGrid = hyper_grid,
metric="RMSE"
)
ggplot(knn_fit)
e
E
# illustrative example of logistic regression
# Hongjie Wang 3-28-2021
# avoid using automation to show basic steps to EJW
rm(list = ls())
setwd("c:/users/hongjie/Documents/data_science/hands-on-ML/logistic_regression")
library(tidyverse)
# col_types is useful in getting hint on var types
# but not practical if you have huge nbr of variables
mydata<-read_csv("donors.csv", col_types = "nnffnnnnnnnnffffffffff")
# change the dep to 0/1 factor
mydata<-mydata %>%
mutate(respondedMailing=as.factor(ifelse(respondedMailing==TRUE,1,0)))
# for illustration,we assume only subset of vars
mydata<-mydata%>%
select(incomeRating,respondedMailing,urbanicity,gender,
age,averageGiftAmount,monthsSinceLastDonation,
numberGifts,totalGivingAmount)
library(rsample)
set.seed(1234)
split_stra<-initial_split(mydata,prop=0.7,strata = respondedMailing)
donor_train<-training(split_stra)
donor_test<-testing(split_stra)
# focus on training dataset
# check out the categorical variables
donor_train %>% select(where(is.factor)) %>%
summary()
# change missing to UNK
donor_train<-donor_train%>%
mutate(urbanicity=as.character(urbanicity)) %>%
mutate(urbanicity=ifelse(is.na(urbanicity),'UNK',urbanicity))%>%
mutate(urbanicity=as.factor(urbanicity))
donor_train <- donor_train%>%
mutate(incomeRating = as.character(incomeRating)) %>%
mutate(incomeRating = as.factor(ifelse(is.na(incomeRating), 'UNK',
incomeRating)))
# gender is a factor, need to convert to charater so that
# we can add UNK, it will
# then convert back to factor
donor_train <- donor_train%>%
mutate(gender=as.character(gender))%>%
mutate(gender = as.factor(ifelse(is.na(gender), 'UNK',
gender)))
#exam the numeric variables
donor_train%>%select(where(is.numeric))%>%summary()
# we will impute age with median of the same gender
donor_train<-donor_train%>%
group_by(gender)%>%
mutate(age=ifelse(is.na(age),median(age,na.rm=TRUE),age))%>%
ungroup()
# donor_train%>%group_by(gender)%>%summarise(m=mean(age),n=sd(age))
# look at distribution of variables
donor_train %>%
select (-respondedMailing) %>%
keep (is.numeric) %>%
gather () %>%
ggplot () +
geom_histogram(mapping = aes(x=value,fill=key), color = "black") +
facet_wrap (~ key, scales = "free") +
theme_minimal ()
# derive outlier cutoff for totalGivingAmount
# set it as max fo this variable
donor_train<-donor_train%>%
mutate(temp1=quantile(totalGivingAmount, .75)
+ (1.5 *IQR(totalGivingAmount)))%>%
mutate(totalGivingAmount=ifelse(totalGivingAmount>=temp1,
temp1,totalGivingAmount))%>%
select(-temp1)
# make sure no excessive multicollinearity
library(stats)
library(corrplot)
donor_train%>%
keep(is.numeric) %>%
cor() %>%
corrplot()
# use smote to oversample
library(performanceEstimation)
donor_train_adj <- smote(respondedMailing ~ .,
donor_train,
perc.over = 1,perc.under=2)
# model
mymod <- glm(data=donor_train_adj, family=binomial,
formula=respondedMailing ~ .)
train_pred_adj <- predict(mymod, donor_train_adj, type = 'response')
summary(train_pred_adj)
train_pred <- predict(mymod, donor_train, type = 'response')
summary(train_pred)
# implementation
# technically, we should find these transformations strictly from training process
#  we do not do that for time constraints
# in automation like recipe or caret, such will be automated
donor_test<-donor_test%>%
mutate(urbanicity=as.character(urbanicity)) %>%
mutate(urbanicity=ifelse(is.na(urbanicity),'UNK',urbanicity))%>%
mutate(urbanicity=as.factor(urbanicity))%>%
mutate(incomeRating = as.character(incomeRating)) %>%
mutate(incomeRating = as.factor(ifelse(is.na(incomeRating), 'UNK',
incomeRating)))%>%
mutate(gender=as.character(gender))%>%
mutate(gender = as.factor(ifelse(is.na(gender), 'UNK',
gender)))
donor_test<-donor_test%>%
group_by(gender)%>%
mutate(age=ifelse(is.na(age),median(age,na.rm=TRUE),age))%>%
ungroup()
donor_test<-donor_test%>%
mutate(temp1=quantile(totalGivingAmount, .75)
+ (1.5 *IQR(totalGivingAmount)))%>%
mutate(totalGivingAmount=ifelse(totalGivingAmount>=temp1,
temp1,totalGivingAmount))%>%
select(-temp1)
# score on test data
test_pred <- predict(mymod, donor_test, type = 'response')
summary(test_pred)
# find optimal cutoff to classify 0 or 1
library(InformationValue)
ideal_cutoff <-optimalCutoff(
actuals = donor_test$respondedMailing,
predictedScores =test_pred,
optimiseFor = "Both")
# misclassification table manually
test_class<-ifelse(test_pred>=ideal_cutoff,1,0)
table(donor_test$respondedMailing,test_class)
donor_matrix<-caret::confusionMatrix(test_class,
donor_test$respondedMailing,
positive=1)
donor_matrix<-caret::confusionMatrix(test_class,
donor_test$respondedMailing)
test_class<-as.factor(test_class)
donor_matrix<-caret::confusionMatrix(test_class,
donor_test$respondedMailing)
donor_matrix
test_class<-as.factor(test_class)
donor_matrix<-caret::confusionMatrix(test_class,
donor_test$respondedMailing,positive=1)
test_class<-as.factor(test_class)
donor_matrix<-caret::confusionMatrix(test_class,
donor_test$respondedMailing,positive="1")
donor_matrix
head(test_pred)
install.packages("ROCR")
roc_pred <-
ROCR::prediction(
predictions = test_pred,
labels = donor_test$respondedMailing
)
roc_pred <-
ROCR::prediction(
predictions = test_pred,
labels = donor_test$respondedMailing
)
roc_perf <- ROCR::performance(roc_pred, measure = "tpr", x.measure = "fpr")
roc_perf
plot(roc_perf, main = "ROC Curve", col = "green", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
ROCR::performance(roc_pred, measure = "auc")
print(ROCR::performance(roc_pred, measure = "auc"))
summary(ROCR::performance(roc_pred, measure = "auc"))
mod_auc<-ROCR::performance(roc_pred, measure = "auc")
unlist(slot(mod_auc,"y.values"))
